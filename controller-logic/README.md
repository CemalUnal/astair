**Machine Learning Model**
---
In order to maximize the overall happiness, we created a model that analyzes user votes and data coming from temperature and humudity sensors. We consider the number of "good" votes as an indicator of happiness. That is the model is trying to reduce the number of "hot" and "cold" votes. In order the to achieve this, we used Reinforcement Learning techniques. 

To explain the process better, first we need to describe the components that the model contains. 

**Data**
1) Temperature and Humudity (from sensors in the room)
2) Outdoor temperature (from weather forecast api)
3) A/C settings (from A/C via RaspberryPI)
4) User votes as "Hot", "Good", "Cold" (from Slack and ASTAIR website)

**Models**
1) SVM model which classifies the weather conditions out of 5.
2) Q-Learning Model (The heart of our reinforcement learning process)

**Aim:**
- The model has a connection to A/C. The aim is adjusting temperature setting of A/C properly by analyzing the data we have.

**Files**
- **ac_control.py** has A/C connection and adjust temperature, fan speed, mode and power.
- **database_helper.py** has database connections. It fetches all the necessary data and creating proper data structure.
- **state_generator.py** has the SVM model mentioned above. Basically it classifies weather conditions according to temperature value. 
- **q_learning.py** is the most critical module. It has Q-Learning model and where all the learning stuff happens. This model will be explained in detail.
- **modelDecision.py** is like a top module where all the data is gathered, transmitted to the machine learning models and sent actions to A/C.
- **temp_map.pkl** is a necessary file to keep previous vote results. It is generated by the program.
- **ac.pkl** is our Q model. It is necessary for keeping progessing the learning process. It is generated by the program.
- **epsilon.pkl** is a hyperparameter of Q-Learning model. It is generated by the program.

**Q-Learning Process**
- Q Learning is one of the most popular reinforcement learning algorithms. Before giving detail about Q-Learning, we need to explain why we choose Reinforcement Learning. Since RL does not require previous data, the model learns gradually during the runtime. Thereby, it can adapt itself according to user feedbacks.
- However, one of the key points in RL is that we need to tell the model it is doing something right or wrong. To do that, we specify reward and punishment. Basically, the algoritm is trying to maximize its reward. 
- On the other hand, we have states and actions. According to the current state, the model decides the next action by looking at the past experiences and trying to maximize its future reward. 
- There is another important issue about RL. Since we do not know the environment in the beginning, first we need to explore the environment. In order to explore, we have to take random actions and get some rewards/punishment. Once we have enough information about the environment, we can decide our action by looking at the experiences. (Note that this may cause some unintended results in A/C case because it can decrease the temperature setting even though it is cold. However it will eventually learn. A simple solution may be we can add some constraints to the model.)
- Okay. We know a little bit about RL and Q-Learning. Now, it is time to embed it in our case. To begin with states, our SVM model just does this. It basically analyzes temperature and gives us the result which is out of 5. 

- (0 - Very Cold)
- (1 - Cold)
- (2 - Good)
- (3 - Hot)
- (4 - Very Hot).
---
According to these states above, the model increases or decreases the temperature. This step forms our actions. 
- (0 - (-2 Decrease))
- (1 - (-1 Decrease))
- (2 - (0 No Change))
- (3 - (+1 Increase))
- (4 - (+2 Increase))
---
- On the other hand, to specify rewards and punishment we use user votes. We consider each vote seperately. That is people are not just numbers, we care the change in their feelings. The process basically consists of comparing each person's previous and current votes. 

If there is no change -> we got 0 reward.

If previous is hot/cold and current is good -> we got 1 reward.

If previous is good and current is hot/cold -> we got -1 reward. (Punishment)

If previous is hot and current is cold (or vice versa) -> we got -1 reward. (Punishment)

If a person has no previous or current vote -> we got 0 reward. (It does not affect the current action)



**Rule Based Models**
---
In this chapter, we will create rule based models for ASTAiR project. These models will dynamically change the A/C degree, according to the parameters that it receives from the work place and user feedbacks. In this way, we aim that thermal comfort will be provided in the work places in a democratic way. We will create two models. These models are "Vote Based Model" and "Predicted Mean Vote (PMV)".

The system analyzes the work places, taking data from a database every 10 minutes. If the mean radiant temperature increases or decreases by 1 degree, the "PMV Model" runs. Otherwise, system gives notice to feedback collector, feedbacks are collected and "Vote Based Model" is run.

**Vote Based Model**
---
This model is based on the results of voting at slack. These voting results will be accessed from the database. The percentage of data received from the database is calculated and received A/C degree and the optimum degree of air conditioning is calculated according to a particular algorithm.

This model created specially for one questionnaire. This question below:
- "How is the temperature inside?" (Multiple Choice Answers: Hot/Good/Cold)

**Input Parameters:**

- Number of "Hot" Vote
- Number of "Good" Vote
- Number of "Cold" Vote
- Instant A/C Degree

**Output Parameters:**

- A/C Degree

**Degree Scale**

This model calculates which option gets the most votes. For instance, it got the most votes "Hot". We created some rules and scales. We decide degree by looking at the vote rate and these conditions. These conditions below that:

- When all votes are the same rate that when hot and cold vote rate are equal or good vote rate is greater than these, A/C degree no change. If good vote rate is not equal these votes and is not greater than these, degree increase/decrease 1 Celsius.
- Cold and good vote rate are equal. Hot vote rate are greater than these, degree decrease 1 Celsius. Otherwise, degree increase 1 Celsius.
- Hot and good vote rate are equal. Hot vote rate are greater than these, degree decrease 1 Celsius. Otherwise, degree increase 1 Celsius.
- Other cases are we follows the these scale:

(HOT)

+4 -> %90

+3 -> %60

+2 -> %40

(+1) <-> (0) <-> (-1)

-2 -> %40

-3 -> %60

-4 -> %90

(COLD)

**Note 1:** (+1), (0) and (-1) values are valid for other three conditions. Thus, these values have no vote rate.

**Note 2:** A/C has some degree ranges. For instance, we have A/C degree ranges: 16 - 30 Celsius, so if system goes beyond these ranges, fixed to the lowest or highest value.

**Predicted Mean Vote (PMV)**
---
The PMV is an index that predicts the mean value of the votes of a large group of persons on the 7-point thermal sensation scale, based on the heat balance of the human body. 

The PPD is an index that establishes a quantitative prediction of the percentage of thermally dissatisfied people who feel too cool or too warm. 

The PMV and PPD are ISO 7730 standard.

**Input Parameters:**

- Air Temperature (Celsius)
- Mean Radiant Temperature (Celsius)
- Relative Air Velocity (m/s)
- Relative Humidity (%)
- Metabolic Rate (met) 
- Clothing (clo)
- External Work (Generally 0)

**Output Parameters:**

- PMV Values
- PPD Values
- A/C Degree

**How to install?**

We will create docker image. When you work this code, you should install docker on personal computer. After installing, you must write the following codes to the console:

- docker build -t models . 
- docker run models

**References**
---
[1] [ISO 7730 Documents](http://www.asandanismanlik.com/wp-content/uploads/2016/12/katalogs-1343-DD.24-TS_EN_ISO_7730.pdf) 
